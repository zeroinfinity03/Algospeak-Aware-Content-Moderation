{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Cell 1: Install Unsloth & QLoRA Dependencies (quiet mode)\n",
    "\n",
    "# 1. Upgrade pip quietly\n",
    "!pip install -q --upgrade pip\n",
    "\n",
    "# 2. Install Unsloth with Colab extras quietly\n",
    "!pip install -q --upgrade unsloth\n",
    "\n",
    "# 3. Install core libraries without re-pulling extras, in quiet mode\n",
    "!pip install -q --no-deps \\\n",
    "    peft>=0.7.0 \\\n",
    "    trl>=0.4.0 \\\n",
    "    accelerate>=0.25.0 \\\n",
    "    datasets>=2.15.0 \\\n",
    "    scipy>=1.11.0 \\\n",
    "    huggingface_hub>=0.19.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Cell 2: Hugging Face Login\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()  # follow the popup to enter your token securely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Cell 3: Load,ize, Inject LoRA & Train with Unsloth on T4 GPU\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "import psutil\n",
    "\n",
    "# ── Configuration ───────────────────────────────────────────────────────────\n",
    "MODEL_NAME   = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "OUTPUT_DIR   = \"qwen-algospeak-unsloth\"\n",
    "DATASET_FILE = \"training_dataset_colab.json\"\n",
    "MAX_SEQ_LEN  = 512\n",
    "\n",
    "# ── 1. Load & Inspect Dataset ───────────────────────────────────────────────\n",
    "def load_large_dataset(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"✅ Loaded {len(data):,} samples\")\n",
    "    algospeak = sum(1 for x in data if x.get(\"is_algospeak\", False))\n",
    "    print(f\"📊 Algospeak samples: {algospeak:,} ({algospeak/len(data)*100:.1f}%)\")\n",
    "    return data\n",
    "\n",
    "training_data = load_large_dataset(DATASET_FILE)\n",
    "\n",
    "# ── 2. 4-bit Model Load via Unsloth ──────────────────────────────────────────\n",
    "print(\"🤖 Loading & quantizing model in 4-bit mode…\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    load_in_4bit=True,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# ── 3. Inject LoRA Adapters ─────────────────────────────────────────────────\n",
    "print(\"🔧 Applying LoRA adapters…\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ── 4. Prepare & Tokenize Dataset ──────────────────────────────────────────\n",
    "def format_prompt(sample):\n",
    "    return f\"\"\"### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "{sample['output']}\"\"\"\n",
    "\n",
    "texts = [format_prompt(s) for s in training_data]\n",
    "dataset = Dataset.from_dict({\"text\": texts}).train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Cleanup large lists\n",
    "del training_data, texts\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    out = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "print(\"🔤 Tokenizing dataset…\")\n",
    "tokenized = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "# ── 5. TrainingArguments & Trainer Setup ────────────────────────────────────\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=2,\n",
    "    # report_to=\"none\",           # UNCOMMENT this line to disable WandB tracking\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ── 6. Start Training ───────────────────────────────────────────────────────\n",
    "print(\"🚀 Starting QLoRA training on massive Algospeak dataset…\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Cell 6: Post-Training Model Export & 4-bit GGUF Conversion\n",
    "print(\"💾 Creating 4-bit GGUF model...\")\n",
    "\n",
    "# ── Configuration ───────────────────────────────────────────────────────────\n",
    "ADAPTER_DIR = \"qwen-algospeak-unsloth\"\n",
    "GGUF_DIR = f\"{ADAPTER_DIR}_gguf_4bit\"\n",
    "\n",
    "# ── 1. Save LoRA Adapters (Small ~100MB) ─────────────────────────────────────\n",
    "print(\"📁 Saving LoRA adapters...\")\n",
    "model.save_pretrained(ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(ADAPTER_DIR)\n",
    "print(f\"✅ LoRA adapters saved to {ADAPTER_DIR}\")\n",
    "\n",
    "# ── 2. Convert to 4-bit GGUF Only ────────────────────────────────────────────\n",
    "print(\"⚙️ Converting to 4-bit GGUF...\")\n",
    "os.makedirs(GGUF_DIR, exist_ok=True)\n",
    "\n",
    "# ONLY 4-bit GGUF (dynamic quantization)\n",
    "model.save_pretrained_gguf(\n",
    "    f\"{GGUF_DIR}/q4_k_m\", \n",
    "    tokenizer, \n",
    "    quantization_method=\"q4_k_m\"\n",
    ")\n",
    "print(\"✅ 4-bit GGUF created\")\n",
    "\n",
    "# ── 3. Memory Cleanup ──────────────────────────────────────────────────────────\n",
    "print(\"🧹 Cleaning up memory...\")\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"✅ 4-bit GGUF model ready for download!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Cell 7: Download LoRA Adapters & 4-bit GGUF\n",
    "from google.colab import files\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# ── Configuration ───────────────────────────────────────────────────────────\n",
    "ADAPTER_DIR = \"qwen-algospeak-unsloth\"\n",
    "GGUF_DIR = f\"{ADAPTER_DIR}_gguf_4bit\"\n",
    "\n",
    "# Zip file names\n",
    "ADAPTER_ZIP = \"lora_adapters.zip\"           # ~100MB\n",
    "GGUF_4BIT_ZIP = \"gguf_4bit_model.zip\"       # ~2GB\n",
    "\n",
    "# ── 1. Zip LoRA Adapters ─────────────────────────────────────────────────────\n",
    "if os.path.isdir(ADAPTER_DIR):\n",
    "    print(f\"📦 Zipping LoRA adapters: {ADAPTER_DIR} → {ADAPTER_ZIP}\")\n",
    "    subprocess.run([\"zip\", \"-r\", ADAPTER_ZIP, ADAPTER_DIR], check=True)\n",
    "    size_mb = os.path.getsize(ADAPTER_ZIP) / (1024*1024)\n",
    "    print(f\"📊 Adapter size: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"❌ Adapter folder not found: {ADAPTER_DIR}\")\n",
    "\n",
    "# ── 2. Zip 4-bit GGUF Model ──────────────────────────────────────────────────\n",
    "if os.path.isdir(GGUF_DIR):\n",
    "    print(f\"📦 Zipping 4-bit GGUF: {GGUF_DIR} → {GGUF_4BIT_ZIP}\")\n",
    "    subprocess.run([\"zip\", \"-r\", GGUF_4BIT_ZIP, GGUF_DIR], check=True)\n",
    "    size_gb = os.path.getsize(GGUF_4BIT_ZIP) / (1024*1024*1024)\n",
    "    print(f\"📊 4-bit GGUF size: {size_gb:.1f} GB\")\n",
    "else:\n",
    "    print(f\"⚠️ No 4-bit GGUF found at {GGUF_DIR}\")\n",
    "\n",
    "# ── 3. Download Both Files ───────────────────────────────────────────────────\n",
    "print(\"\\n🎯 Downloading:\")\n",
    "print(\"1. 📱 LoRA Adapters (~100MB)\")\n",
    "print(\"2. 🚀 4-bit GGUF Model (~2GB) - Ready for Ollama/LM Studio\")\n",
    "\n",
    "print(\"\\n⬇️ Starting downloads...\")\n",
    "files.download(ADAPTER_ZIP)\n",
    "if os.path.exists(GGUF_4BIT_ZIP):\n",
    "    files.download(GGUF_4BIT_ZIP)\n",
    "\n",
    "print(\"✅ Downloads complete! 🎉\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
