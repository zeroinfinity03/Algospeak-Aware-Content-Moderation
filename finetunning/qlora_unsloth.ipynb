{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Cell 1: Install Unsloth & QLoRA Dependencies (quiet mode)\n",
    "\n",
    "# 1. Upgrade pip quietly\n",
    "!pip install -q --upgrade pip\n",
    "\n",
    "# 2. Install Unsloth with Colab extras quietly\n",
    "!pip install -q --upgrade unsloth\n",
    "\n",
    "# 3. Install core libraries without re-pulling extras, in quiet mode\n",
    "!pip install -q --no-deps \\\n",
    "    peft>=0.7.0 \\\n",
    "    trl>=0.4.0 \\\n",
    "    accelerate>=0.25.0 \\\n",
    "    datasets>=2.15.0 \\\n",
    "    scipy>=1.11.0 \\\n",
    "    huggingface_hub>=0.19.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Cell 2: Hugging Face Login\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()  # follow the popup to enter your token securely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Cell 3: Load,ize, Inject LoRA & Train with Unsloth on T4 GPU\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "import psutil\n",
    "\n",
    "# ── Configuration ───────────────────────────────────────────────────────────\n",
    "MODEL_NAME   = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "OUTPUT_DIR   = \"qwen-algospeak-unsloth\"\n",
    "DATASET_FILE = \"training_dataset_colab.json\"\n",
    "MAX_SEQ_LEN  = 512\n",
    "\n",
    "# ── 1. Load & Inspect Dataset ───────────────────────────────────────────────\n",
    "def load_large_dataset(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"✅ Loaded {len(data):,} samples\")\n",
    "    algospeak = sum(1 for x in data if x.get(\"is_algospeak\", False))\n",
    "    print(f\"📊 Algospeak samples: {algospeak:,} ({algospeak/len(data)*100:.1f}%)\")\n",
    "    return data\n",
    "\n",
    "training_data = load_large_dataset(DATASET_FILE)\n",
    "\n",
    "# ── 2. 4-bit Model Load via Unsloth ──────────────────────────────────────────\n",
    "print(\"🤖 Loading & quantizing model in 4-bit mode…\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    load_in_4bit=True,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# ── 3. Inject LoRA Adapters ─────────────────────────────────────────────────\n",
    "print(\"🔧 Applying LoRA adapters…\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# ── 4. Prepare & Tokenize Dataset ──────────────────────────────────────────\n",
    "def format_prompt(sample):\n",
    "    return f\"\"\"### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "{sample['output']}\"\"\"\n",
    "\n",
    "texts = [format_prompt(s) for s in training_data]\n",
    "dataset = Dataset.from_dict({\"text\": texts}).train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Cleanup large lists\n",
    "del training_data, texts\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    out = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "print(\"🔤 Tokenizing dataset…\")\n",
    "tokenized = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True)\n",
    "\n",
    "# ── 5. TrainingArguments & Trainer Setup ────────────────────────────────────\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=2,\n",
    "    # report_to=\"none\",           # UNCOMMENT this line to disable WandB tracking\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ── 6. Start Training ───────────────────────────────────────────────────────\n",
    "print(\"🚀 Starting QLoRA training on massive Algospeak dataset…\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import subprocess, os\n",
    "\n",
    "ADAPTER_DIR      = \"qwen-algospeak-unsloth\"\n",
    "MERGED_DIR       = f\"{ADAPTER_DIR}_merged\"\n",
    "ADAPTER_ZIP      = \"adapters.zip\"\n",
    "MERGED_MODEL_ZIP = \"merged_model.zip\"\n",
    "\n",
    "# Zip adapters\n",
    "if os.path.isdir(ADAPTER_DIR):\n",
    "    print(f\"📦 Zipping {ADAPTER_DIR} → {ADAPTER_ZIP}\")\n",
    "    subprocess.run([\"zip\",\"-r\",ADAPTER_ZIP,ADAPTER_DIR], check=True)\n",
    "else:\n",
    "    raise FileNotFoundError(ADAPTER_DIR)\n",
    "\n",
    "# Zip merged model (if exists)\n",
    "if os.path.isdir(MERGED_DIR):\n",
    "    print(f\"📦 Zipping {MERGED_DIR} → {MERGED_MODEL_ZIP}\")\n",
    "    subprocess.run([\"zip\",\"-r\",MERGED_MODEL_ZIP,MERGED_DIR], check=True)\n",
    "\n",
    "# Download\n",
    "print(\"⬇️ Downloading…\")\n",
    "files.download(ADAPTER_ZIP)\n",
    "if os.path.exists(MERGED_MODEL_ZIP):\n",
    "    files.download(MERGED_MODEL_ZIP)\n",
    "print(\"✅ Step 4 complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
