TrustLab logo
TrustLab
Share
Show more options
AI Engineer – LLM-Based Content Moderation
Palo Alto, CA · 3 months ago · Over 100 people clicked apply
Responses managed off LinkedIn


$120K/yr - $200K/yr

 Hybrid
Matches your job preferences, workplace type is Hybrid.

 Full-time
Matches your job preferences, job type is Full-time.

Apply

Save
Save AI Engineer – LLM-Based Content Moderation at TrustLab
Your AI-powered job assessment


Am I a good fit?

Tailor my resume

How can I best position myself?
About the job
Who we are:

Our mission is to make the internet safer and more enjoyable for everyone. We combine state of the art AI technology and human judgment for best in class content detection, harm mitigation and safety monitoring. We are a VC-backed startup, founded by senior Google, YouTube, TikTok and Reddit executives, working with some of the world’s largest online platforms, and fast-moving startups. If you are looking for an opportunity to apply AI technology to real-world business use cases at a significant scale, and an opportunity to shape the future of how we can safely enjoy user generated online content, we’d love to hear from you.

What you’ll do:

As an AI Engineer you will design, build and tune industry leading classifiers for digital content using LLMs, and contribute to training, evaluation, and monitoring new and improved LLMs and other algorithmic models. You will also contribute to the development of the infrastructure for efficient experimentation, as well as product deployment on our cloud based infrastructure. As an integral part of the team, you will propose and test new ideas for groundbreaking new approaches that combine human judgement and LLM reasoning to elevate the industry’s approach to content understanding and moderation.

Key Responsibilities:

Design, develop, and optimize AI models for content moderation, focusing on precision and recall improvements. 
Fine-tune LLMs for classification tasks related to abuse detection, leveraging supervised and reinforcement learning techniques. 
Optimize model performance through advanced techniques such as active learning, self-supervision, and domain adaptation. 
Deploy and monitor content moderation models in production, iterating based on real-world performance metrics and feedback loops. 

What we’re looking for:

Bachelor's or Master’s degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field. 
Experience in AI/ML, with a focus on NLP, deep learning, and LLMs. 
Proficiency in Python and deep learning frameworks such as TensorFlow, PyTorch, or JAX. 
Experience in fine-tuning and deploying transformer-based models like GPT, BERT, T5, or similar. 
Familiarity with evaluation metrics for classification tasks (e.g., F1-score, precision-recall curves) and best practices for handling imbalanced datasets. 

Preferred Qualifications:

Experience working with large-scale, real-world content moderation datasets. 
Knowledge of regulatory frameworks related to content moderation (e.g., GDPR, DSA, Section 230). 
Familiarity with knowledge distillation and model compression techniques for efficient deployment. 
Experience with reinforcement learning (e.g., RLHF) for AI safety applications. 

Why Join Us?

Work with a group of renown industry leaders in AI and Online Safety to shape the future of the industry. 
Ample opportunity and support for growth, as a technical individual contributor, or manager. 
Apply AI technology to real-world business use cases at a significant scale, with blue chip customers
Work as part of a team where you can know everyone, but don’t have to do everyone’s job. 
Competitive compensation, comprehensive benefits, and hybrid in-office policy. 

The pay range for this role is:

120,000 - 200,000 USD per year (Palo Alto)


