{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89498b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete Package Installation for QLoRA Fine-tuning\n",
    "Run this first in your Colab notebook\n",
    "\"\"\"\n",
    "\n",
    "# Install required packages for QLoRA training\n",
    "install_commands = [\n",
    "    \"pip install -q transformers>=4.45.0\",\n",
    "    \"pip install -q peft>=0.7.0\", \n",
    "    \"pip install -q bitsandbytes>=0.42.0\",\n",
    "    \"pip install -q accelerate>=0.25.0\",\n",
    "    \"pip install -q datasets>=2.15.0\",\n",
    "    \"pip install -q scipy>=1.11.0\",\n",
    "    \"pip install -q huggingface_hub>=0.19.0\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Installing QLoRA Dependencies...\")\n",
    "for cmd in install_commands:\n",
    "    print(f\"Running: {cmd}\")\n",
    "    import subprocess\n",
    "    result = subprocess.run(cmd.split(), capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ {cmd.split()[2]} installed successfully\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {result.stderr}\")\n",
    "\n",
    "print(\"\\nüîê HuggingFace Login Required:\")\n",
    "print(\"Run this after installation:\")\n",
    "print(\"from huggingface_hub import login\")\n",
    "print(\"login()\")  # You'll enter your HF token here\n",
    "\n",
    "print(\"\\n‚úÖ Installation complete! Ready for QLoRA training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8fce73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363284d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Login (REQUIRED)\n",
    "from huggingface_hub import login\n",
    "login()  # You'll get a popup to enter your HF token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21bd870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ad733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Clean QLoRA Fine-tuning for Massive Algospeak Dataset\n",
    "Optimized for L4 GPU with 79k+ samples\n",
    "All fixes included: labels for tokenization, WandB documentation, no duplicates\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "\n",
    "# Configuration for massive dataset\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "OUTPUT_DIR = \"qwen-algospeak-lora\"\n",
    "DATASET_FILE = \"training_dataset_colab.json\"\n",
    "\n",
    "print(\"üöÄ Loading Massive Algospeak Dataset...\")\n",
    "\n",
    "# Memory-efficient dataset loading\n",
    "def load_large_dataset(file_path):\n",
    "    \"\"\"Load large JSON dataset efficiently\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå Dataset not found: {file_path}\")\n",
    "        print(\"üìã Please upload your training_dataset_colab.json file!\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded: {len(data):,} samples\")\n",
    "    \n",
    "    # Check data quality\n",
    "    algospeak_count = sum(1 for item in data if item.get('is_algospeak', False))\n",
    "    print(f\"üìä Algospeak samples: {algospeak_count:,} ({algospeak_count/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "training_data = load_large_dataset(DATASET_FILE)\n",
    "if not training_data:\n",
    "    print(\"‚ùå Cannot proceed without valid training data\")\n",
    "    raise SystemExit(\"Please upload your dataset file\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"üìù Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Optimized 4-bit quantization for L4\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model with optimal settings\n",
    "print(\"ü§ñ Loading Qwen2.5-3B with 4-bit quantization...\")\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False,\n",
    "        attn_implementation=\"flash_attention_2\",  # Try Flash Attention 2\n",
    "    )\n",
    "    print(\"‚úÖ Flash Attention 2 enabled!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Flash Attention 2 not available: {e}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False,\n",
    "    )\n",
    "    print(\"‚úÖ Standard attention enabled\")\n",
    "\n",
    "# Prepare for efficient training\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "# Optimized LoRA config for algospeak detection\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Good balance for accuracy\n",
    "    lora_alpha=32,          # 2x rank for stability  \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,      # Light dropout\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"üîß Trainable parameters:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Format for algospeak detection\n",
    "def format_algospeak_prompt(sample):\n",
    "    \"\"\"Format with algospeak context\"\"\"\n",
    "    return f\"\"\"### Instruction:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Response:\n",
    "{sample['output']}\"\"\"\n",
    "\n",
    "# Process massive dataset efficiently\n",
    "print(\"üîÑ Processing massive dataset...\")\n",
    "formatted_data = []\n",
    "for i, sample in enumerate(training_data):\n",
    "    formatted_data.append({\"text\": format_algospeak_prompt(sample)})\n",
    "    \n",
    "    # Progress indicator for large dataset\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"Processed {i+1:,}/{len(training_data):,} samples...\")\n",
    "\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"üìö Train: {len(dataset['train']):,} | Test: {len(dataset['test']):,}\")\n",
    "\n",
    "# Memory cleanup\n",
    "del formatted_data, training_data\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Efficient tokenization - FIXED VERSION\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=512,  # Optimal for algospeak content\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    # For causal LM, labels = input_ids (this is critical!)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "print(\"üî§ Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    batch_size=1000,  # Process in batches for efficiency\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# üìä ABOUT WANDB (Weights & Biases) - EXPERIMENT TRACKING\n",
    "\"\"\"\n",
    "ü§î What is WandB?\n",
    "WandB is an experiment tracking platform that creates beautiful dashboards for your training:\n",
    "- üìà Real-time loss curves and training graphs  \n",
    "- üíæ Automatic model checkpoint management\n",
    "- üìä GPU usage, memory consumption tracking\n",
    "- üîÑ Compare different training runs\n",
    "\n",
    "üéØ Do You Need It?\n",
    "‚ùå NO for basic training - You just want the model trained\n",
    "‚úÖ YES for research - You want detailed monitoring and graphs\n",
    "\n",
    "üîß How to Control WandB:\n",
    "- ENABLE:  Add report_to=\"wandb\" below (requires WandB account + API key)\n",
    "- DISABLE: Add report_to=\"none\" below (simple terminal output only)\n",
    "- DEFAULT: No report_to parameter = WandB will try to initialize automatically\n",
    "\n",
    "üí° RECOMMENDATION: Add report_to=\"none\" for simple training without extra setup\n",
    "\"\"\"\n",
    "\n",
    "# L4-optimized training arguments  \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,      # Conservative for stability\n",
    "    per_device_eval_batch_size=4,       # Larger for eval\n",
    "    gradient_accumulation_steps=8,      # Effective batch = 16\n",
    "    num_train_epochs=3,                 # Good for large dataset\n",
    "    learning_rate=1e-4,                 # Optimal for QLoRA\n",
    "    fp16=True,                          # Memory optimization\n",
    "    logging_steps=100,                  # More frequent logging\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,                     # Less frequent eval (large dataset)\n",
    "    save_steps=1000,                    # Save checkpoints\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,        # Better for Colab\n",
    "    gradient_checkpointing=True,        # Memory optimization\n",
    "    warmup_steps=100,                   # Smooth training start\n",
    "    # report_to=\"none\",                 # üîß UNCOMMENT this line to disable WandB tracking\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting QLoRA training on massive algospeak dataset...\")\n",
    "print(f\"üìä Training on {len(tokenized_dataset['train']):,} samples\")\n",
    "print(f\"üéØ Target: High-accuracy algospeak detection\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"üíæ Saving fine-tuned model...\")\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"üéâ QLoRA Fine-tuning Complete!\")\n",
    "print(f\"üìÅ Model saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Quick test of the fine-tuned model\n",
    "print(\"\\nüß™ Testing fine-tuned model...\")\n",
    "test_prompt = \"\"\"### Instruction:\n",
    "Analyze this content for harmfulness and classify it appropriately. Consider both direct language and coded language (algospeak).\n",
    "\n",
    "### Input:\n",
    "I want to unalive myself\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Quick inference test\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "test_response = response.split(\"### Response:\")[-1].strip()\n",
    "print(f\"üìù Test Input: 'I want to unalive myself'\")\n",
    "print(f\"ü§ñ Model Response: {test_response}\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready for algospeak detection!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794d74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download adapters to your Mac\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Create zip of adapter files\n",
    "!zip -r qwen_algospeak_adapters.zip qwen-algospeak-lora/\n",
    "\n",
    "# Download the zip file\n",
    "files.download('qwen_algospeak_adapters.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05185d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eecbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
