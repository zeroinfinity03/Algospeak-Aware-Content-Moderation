#!/usr/bin/env python3
"""
Master Script: Complete QLoRA â†’ GGUF â†’ Ollama Workflow
Orchestrates the entire fine-tuning and deployment pipeline
"""

import subprocess
import sys
import os
from pathlib import Path

def run_step(script_name, step_name):
    """Run a workflow step"""
    print(f"\n{'='*60}")
    print(f"ğŸš€ {step_name}")
    print(f"ğŸ“„ Running: {script_name}")
    print('='*60)
    
    if not Path(script_name).exists():
        print(f"âŒ Script not found: {script_name}")
        return False
    
    try:
        result = subprocess.run([sys.executable, script_name], check=True)
        print(f"âœ… {step_name} completed successfully!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {step_name} failed with error: {e}")
        return False
    except KeyboardInterrupt:
        print(f"\nâš ï¸  {step_name} interrupted by user")
        return False

def check_prerequisites():
    """Check if prerequisites are met"""
    print("ğŸ” Checking prerequisites...")
    
    # Check if running on Mac
    if sys.platform != "darwin":
        print("âš ï¸  This workflow is optimized for macOS")
    
    # Check Python version
    if sys.version_info < (3, 8):
        print("âŒ Python 3.8+ required")
        return False
    
    # Check if files exist
    required_files = [
        "1_download_adapters.py",
        "2_merge_adapters.py", 
        "3_convert_to_gguf.py",
        "4_setup_ollama.py"
    ]
    
    missing_files = []
    for file in required_files:
        if not Path(file).exists():
            missing_files.append(file)
    
    if missing_files:
        print(f"âŒ Missing files: {', '.join(missing_files)}")
        return False
    
    print("âœ… Prerequisites check passed!")
    return True

def show_colab_instructions():
    """Show Colab training instructions"""
    print("\n" + "="*60)
    print("ğŸ“‹ STEP 1: COLAB TRAINING INSTRUCTIONS")
    print("="*60)
    
    print("""
ğŸ”¥ COLAB SETUP (Complete this first):

1. Upload these files to Colab:
   â€¢ install_packages.py
   â€¢ colab_ready_qlora.py
   â€¢ training_dataset_colab.json (your 79k samples)

2. Run in Colab:
   exec(open('install_packages.py').read())

3. Login to HuggingFace:
   from huggingface_hub import login
   login()  # Enter your HF token

4. Start training:
   exec(open('colab_ready_qlora.py').read())

5. After training, download adapters:
   !zip -r qwen_algospeak_adapters.zip qwen-algospeak-lora/
   from google.colab import files
   files.download('qwen_algospeak_adapters.zip')

6. Move the downloaded zip to this directory, then continue below.
""")
    
    input("\nâœ… Press Enter when Colab training is complete and zip is downloaded...")

def main():
    print("ğŸ¯ Complete QLoRA â†’ GGUF â†’ Ollama Workflow")
    print("This will guide you through the entire pipeline")
    
    # Check prerequisites
    if not check_prerequisites():
        print("âŒ Prerequisites not met. Please fix and try again.")
        return
    
    # Show Colab instructions
    show_colab_instructions()
    
    # Step 2: Download and extract adapters
    print("\nğŸš€ Starting Mac-side workflow...")
    if not run_step("1_download_adapters.py", "STEP 2: Download Adapters"):
        print("âŒ Cannot proceed without adapters")
        return
    
    # Step 3: Merge adapters
    if not run_step("2_merge_adapters.py", "STEP 3: Merge Adapters"):
        print("âŒ Cannot proceed without merged model")
        return
    
    # Step 4: Convert to GGUF
    if not run_step("3_convert_to_gguf.py", "STEP 4: Convert to GGUF"):
        print("âŒ Cannot proceed without GGUF model")
        return
    
    # Step 5: Setup Ollama
    if not run_step("4_setup_ollama.py", "STEP 5: Setup Ollama"):
        print("âŒ Ollama setup failed")
        return
    
    # Success!
    print("\n" + "="*60)
    print("ğŸ‰ WORKFLOW COMPLETE!")
    print("="*60)
    
    print("""
âœ… Your fine-tuned algospeak detection model is now deployed!

ğŸ§ª Test your model:
   ollama run qwen-algospeak "I want to unalive myself"

ğŸ”— FastAPI Integration:
   Use ollama_client.py in your main.py

ğŸ“Š Expected performance:
   â€¢ Model size: ~1.8GB
   â€¢ RAM usage: ~2-3GB
   â€¢ Speed: ~15-25 tokens/sec
   â€¢ Accuracy: 95%+ algospeak detection

ğŸš€ Ready for production deployment!
""")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸  Workflow interrupted by user")
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        print("Check the logs and try running individual steps") 